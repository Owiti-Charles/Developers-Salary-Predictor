{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üíº Capstone Project: Predicting Developer Salaries Using Stack Overflow Survey Data (2023‚Äì2024)**\n",
    "\n",
    "---\n",
    "\n",
    "## **üß† Business Understanding**\n",
    "\n",
    "### üéØ Project Objective\n",
    "Build a machine learning model that predicts annual developer salaries using demographic, professional, and technical features. We leverage Stack Overflow‚Äôs 2023 and 2024 survey datasets to inform both global and localized (Kenyan) compensation insights.\n",
    "\n",
    "### ‚ùì Why This Matters\n",
    "Salary transparency is limited in many global regions ‚Äî especially across Africa. Developers often navigate job transitions, promotions, or freelance pricing without reliable benchmarks. This model aims to provide data-driven insights that help both job seekers and hiring managers make informed compensation decisions.\n",
    "\n",
    "This model helps:\n",
    "- Developers **benchmark expected compensation**\n",
    "- Employers **set fair, competitive pay**\n",
    "- Career changers **evaluate the ROI of learning paths**\n",
    "- HR platforms **integrate salary prediction engines**\n",
    "- Policymakers and analysts **understand wage trends in tech**\n",
    "\n",
    "### üåç Kenya-Specific Relevance\n",
    "In Kenya, tech hubs like Nairobi are booming ‚Äî yet salary data remains fragmented. This model could:\n",
    "- Help junior/mid-level developers negotiate better\n",
    "- Empower remote-first hiring with global salary range visibility\n",
    "- Be integrated into job platforms like **Fuzu**, **BrighterMonday**, or **Andela**\n",
    "\n",
    "### üßë‚Äçüíª Industry Domains\n",
    "- **Primary**: Technology\n",
    "- **Secondary**: Human Capital Analytics, Labor Market Research\n",
    "\n",
    "### üë• Stakeholders\n",
    "\n",
    "| Stakeholder Group         | Value Proposition                                                                 |\n",
    "|---------------------------|------------------------------------------------------------------------------------|\n",
    "| Developers                | Benchmark realistic compensation based on skills and experience                   |\n",
    "| Employers & Recruiters    | Offer data-driven, competitive salaries                                            |\n",
    "| HR Tech Platforms         | Integrate model into job boards or career guidance tools                          |\n",
    "| Bootcamps & Career Coaches| Showcase expected returns on upskilling efforts                                   |\n",
    "| Policy & Advocacy Groups  | Inform labor market planning and economic inclusion initiatives                   |\n",
    "\n",
    "### üî¨ Literature & Prior Work\n",
    "This project builds on:\n",
    "- Previous Stack Overflow Salary Calculators (now deprecated)\n",
    "- ML-based salary models using Random Forest, XGBoost, etc.\n",
    "- Global developer reports (GitHub Octoverse, Dev.to, HackerRank)\n",
    "\n",
    "### ‚ú® Unique Contribution\n",
    "- Combines recent **multi-year (2023 & 2024)** datasets\n",
    "- Applies **localized lens** for Kenya/Africa\n",
    "- Prioritizes **real-world use cases** for talent, HR, and learning ecosystems\n",
    "\n",
    "### üéØ Success Metrics\n",
    "- Overall RMSE: < 0.45 (log scale)\n",
    "- SSA/Kenya RMSE: < 0.45\n",
    "- R¬≤ Score: > 0.4\n",
    "- Validation: Use 20% holdout set and 5-fold stratified CV, validated against Kenya salary ranges from local job boards.\n",
    "\n",
    "### ‚úÖ Validation Approach\n",
    "- Reserve 20% of data as a holdout set for final evaluation.\n",
    "- Use stratified 5-fold CV to ensure SSA/Kenya representation.\n",
    "- Cross-check predictions with Kenya salary data from Fuzu or Andela.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìä Data Understanding**\n",
    "\n",
    "### üìÅ Data Sources\n",
    "- `survey_results_public_2023.csv`\n",
    "- `survey_results_public_2024.csv`\n",
    "\n",
    "Source: [Stack Overflow Developer Survey](https://insights.stackoverflow.com/survey)\n",
    "\n",
    "### üìê Data Summary\n",
    "\n",
    "| Year | Total Responses | Countries | SSA Responses | Kenya Responses |\n",
    "|------|------------------|-----------|----------------|------------------|\n",
    "| 2023 | ~89,000          | ~180      | 1,828          | 244              |\n",
    "| 2024 | ~65,000          | ~188      | 1,271          | 180              |\n",
    "\n",
    "Combined, we expect **100,000+ usable rows** across ~188 countries.\n",
    "\n",
    "### üîë Key Features\n",
    "\n",
    "| Category        | Variables (examples)                                                  |\n",
    "|----------------|------------------------------------------------------------------------|\n",
    "| Demographics    | `Age`, `Country`, `Gender`                                             |\n",
    "| Education       | `EdLevel`, `LearnCode`, `YearsCodePro`                                 |\n",
    "| Employment      | `Employment`, `OrgSize`, `RemoteWork`                                  |\n",
    "| Technical Tools | `LanguageHaveWorkedWith`, `PlatformHaveWorkedWith`, `AISelect`         |\n",
    "| Target Variable | `ConvertedCompYearly` (Annual salary in USD)                           |\n",
    "| Meta Info       | `SurveyYear`, `Currency`                                               |\n",
    "\n",
    "### ‚ö†Ô∏è Data Limitations & Mitigation Strategies\n",
    "\n",
    "| Limitation                             | Proposed Mitigation                                           |\n",
    "|----------------------------------------|---------------------------------------------------------------|\n",
    "| Schema changes between years           | Focus on **common columns**; map others as needed             |\n",
    "| Imbalanced country distribution        | Consider **oversampling**, stratified models                  |\n",
    "| Salary skew and outliers               | Apply **log transformation**, drop outliers via IQR/z-score   |\n",
    "| Sparse Kenya data                      | Enrich with African subset or train global model              |\n",
    "| Missing values                         | Drop, impute, or bin categories during preprocessing          |\n",
    "\n",
    "### üíæ Planned Enhancements\n",
    "- Add `SurveyYear` column to track year-based differences\n",
    "- Combine datasets using common schema\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "# warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# data handling\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# explainability\n",
    "import shap\n",
    "# utilities\n",
    "import os\n",
    "from pathlib import Path\n",
    "# feature selection and importance\n",
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load 2023 & 2024 Stack Overflow Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the 2023 and 2024 datasets\n",
    "df_2023 = pd.read_csv('survey_results_public_2023.csv')\n",
    "df_2024 = pd.read_csv('survey_results_public_2024.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Understand Schema Differences Between the Two Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_2023.columns.difference(df_2024.columns))\n",
    "print(df_2024.columns.difference(df_2023.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stack Overflow's survey questions vary slightly each year. We checked for columns that appear in one year but not the other. This helps us align the two datasets before combining them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2023.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Merge Datasets for a Unified View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SurveyYear column to distinguish entries\n",
    "df_2023['SurveyYear'] = 2023\n",
    "df_2024['SurveyYear'] = 2024\n",
    "\n",
    "# Align column structure by filling in missing columns in each dataset\n",
    "missing_2023 = df_2024.columns.difference(df_2023.columns)\n",
    "missing_2024 = df_2023.columns.difference(df_2024.columns)\n",
    "\n",
    "# Add missing columns to each dataframe and fill with NaN\n",
    "for col in missing_2023:\n",
    "    df_2023[col] = np.nan\n",
    "\n",
    "for col in missing_2024:\n",
    "    df_2024[col] = np.nan\n",
    "\n",
    "# Reorder columns to match, important for concat\n",
    "df_2023 = df_2023[df_2024.columns]\n",
    "\n",
    "# Concatenate\n",
    "df_all = pd.concat([df_2023, df_2024], ignore_index=True)\n",
    "\n",
    "print(f\"Merged shape: {df_all.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combined the 2023 and 2024 survey responses into one large dataset. To make this possible, we added any missing questions from one year into the other using empty values (NaNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List of categorical features to analyze\n",
    "categorical_features = ['Country', 'Employment', 'RemoteWork', 'OrgSize', 'ICorPM', \n",
    "                        'DevType', 'EdLevel', 'Industry', 'PurchaseInfluence',\n",
    "                        'LanguageHaveWorkedWith', 'PlatformHaveWorkedWith', \n",
    "                        'ToolsTechHaveWorkedWith', 'SurveyYear']\n",
    "\n",
    "# Loop through each feature and plot the top categories\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    # Filter out 'Missing' values first\n",
    "    df_filtered = df_all[df_all[feature] != 'Missing']\n",
    "\n",
    "    # Get top categories by frequency\n",
    "    top_categories = df_filtered[feature].value_counts().nlargest(10).index\n",
    "\n",
    "    # Filter to top categories\n",
    "    filtered_df = df_filtered[df_filtered[feature].isin(top_categories)]\n",
    "\n",
    "    # Compute mean salary per category, sorted by salary\n",
    "    sorted_order = (\n",
    "        filtered_df.groupby(feature)['ConvertedCompYearly']\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "        .index\n",
    "    )\n",
    "\n",
    "    # Create barplot\n",
    "    sns.barplot(\n",
    "        data=filtered_df,\n",
    "        x=feature,\n",
    "        y='ConvertedCompYearly',\n",
    "        estimator=np.mean,\n",
    "        ci=None,\n",
    "        order=sorted_order,\n",
    "        palette='viridis'\n",
    "    )\n",
    "\n",
    "    plt.title(f'Average Salary by {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Average Salary (USD)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "clean_salary = df_all['ConvertedCompYearly'].dropna()\n",
    "clean_salary = clean_salary[clean_salary > 1000]  # Remove very low outliers\n",
    "clean_salary = clean_salary[clean_salary < 300000]  # Cap extreme values\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(clean_salary, bins=50, kde=True, color='purple')\n",
    "plt.axvline(clean_salary.median(), color='red', linestyle='--', label=f\"Median: ${int(clean_salary.median()):,}\")\n",
    "plt.title(\"Distribution of Developer Salaries (USD)\")\n",
    "plt.xlabel(\"Salary (USD)\")\n",
    "plt.ylabel(\"Number of Developers\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Interpretation  \n",
    "Salaries tend to cluster between $30,000 and $100,000 per year, with a few very high earners pulling the average upward. This is known as a right-skewed distribution, most developers earn within a moderate range, while a few outliers earn significantly more.   \n",
    "**Consider applying log transformation during modeling.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant features for salary prediction (features selected using relevance and domain intuition)\n",
    "selected_features = [\n",
    "    'Country', 'Employment', 'RemoteWork', 'OrgSize', 'ICorPM',\n",
    "    'YearsCodePro', 'DevType', 'EdLevel', 'Industry', \n",
    "    'PurchaseInfluence', 'LanguageHaveWorkedWith', 'PlatformHaveWorkedWith',\n",
    "    'ToolsTechHaveWorkedWith', 'ConvertedCompYearly', 'SurveyYear', 'AIThreat'\n",
    "]\n",
    "\n",
    "# Subset and copy data\n",
    "df = df_all[selected_features].copy()\n",
    "\n",
    "# Drop missing or unrealistic target values\n",
    "df = df[df['ConvertedCompYearly'].notna()]\n",
    "df = df[df['ConvertedCompYearly'] > 1000]  # Filter extremely low salaries\n",
    "\n",
    "# Handle missing categorical values\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "df[categorical_cols] = df[categorical_cols].fillna('Missing')\n",
    "\n",
    "# Clean numeric features\n",
    "df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n",
    "df['YearsCodePro'] = df['YearsCodePro'].fillna(df['YearsCodePro'].median())\n",
    "\n",
    "print(f\"‚úÖ Final Cleaned Data Shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Salary by years of professional experience\n",
    "# Filter: Remove missing or unrealistic salary/experience\n",
    "salary_exp_df = df[\n",
    "    df['ConvertedCompYearly'].notna() & \n",
    "    df['YearsCodePro'].notna()\n",
    "].copy()\n",
    "\n",
    "# Bin YearsCodePro for clarity\n",
    "salary_exp_df['ExperienceBin'] = pd.cut(\n",
    "    salary_exp_df['YearsCodePro'],\n",
    "    bins=[0, 2, 5, 10, 15, 20, 30, 50],\n",
    "    labels=['0‚Äì2', '3‚Äì5', '6‚Äì10', '11‚Äì15', '16‚Äì20', '21‚Äì30', '30+']\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(data=salary_exp_df, x='ExperienceBin', y='ConvertedCompYearly', showfliers=False)\n",
    "plt.title('Salary by Years of Professional Experience')\n",
    "plt.xlabel('Years of Coding Experience (Binned)')\n",
    "plt.ylabel('Annual Salary (USD)')\n",
    "plt.ylim(0, salary_exp_df['ConvertedCompYearly'].quantile(0.95))  # Remove extreme outliers\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows a clear positive correlation between years of professional experience and annual salary. \n",
    "As experience increases, salaries tend to rise steadily.  \n",
    "Diminishing returns observed after 20+ years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df.select_dtypes(include=[np.number])\n",
    "corr_matrix = numeric_features.corr()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.heatmap(\n",
    "    corr_matrix[['ConvertedCompYearly']].sort_values(by='ConvertedCompYearly', ascending=False),\n",
    "    annot=True, cmap='viridis', vmin=-1, vmax=1\n",
    ")\n",
    "plt.title(\"Correlation of Numerical Features with Salary\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Salary by ICorPM (Individual Contributor vs People Manager)\n",
    "# Filter to remove missing or \"Missing\" values\n",
    "ic_pm_df = df[\n",
    "    df['ConvertedCompYearly'].notna() &\n",
    "    df['ICorPM'].notna() &\n",
    "    (df['ICorPM'] != 'Missing')\n",
    "].copy()\n",
    "\n",
    "# Plot salary distribution by ICorPM\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(\n",
    "    data=ic_pm_df,\n",
    "    x='ICorPM',\n",
    "    y='ConvertedCompYearly',\n",
    "    showfliers=False  # hides extreme outliers\n",
    ")\n",
    "\n",
    "plt.title('Salary by Role Type: Individual Contributor vs People Manager')\n",
    "plt.xlabel('Role Type')\n",
    "plt.ylabel('Annual Salary (USD)')\n",
    "plt.ylim(0, ic_pm_df['ConvertedCompYearly'].quantile(0.95))  # limit y-axis to 95th percentile\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average annual salary for a People Manager role is significantly higher than the average salary for an Individual Contributor role. The graph shows a clear gap between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salary distribution by country (Top 10)\n",
    "# Get top 10 countries by frequency\n",
    "top_countries = df['Country'].value_counts().head(10).index\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.boxplot(\n",
    "    data=df[df['Country'].isin(top_countries)],\n",
    "    x='Country',\n",
    "    y='ConvertedCompYearly'\n",
    ")\n",
    "\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.ylim(0, 300000) \n",
    "plt.title(\"Salary Distribution by Country (Top 10)\")\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Yearly Compensation (USD)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph illustrates significant differences in the salary ranges between the top 10 countries. Some countries like the United States and Canada have a much wider salary distribution, while others like Poland and India have a more compressed range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with multiple selections\n",
    "tech_columns = ['LanguageHaveWorkedWith', 'PlatformHaveWorkedWith', 'ToolsTechHaveWorkedWith']\n",
    "\n",
    "for col in tech_columns:\n",
    "    # Filter out missing entries\n",
    "    tech_df = df[df[col].notna()].copy()\n",
    "    tech_df = tech_df[tech_df[col] != 'Missing']\n",
    "\n",
    "    # Split and explode\n",
    "    tech_df[col] = tech_df[col].str.split(';')\n",
    "    tech_df = tech_df.explode(col)\n",
    "    tech_df[col] = tech_df[col].str.strip()\n",
    "\n",
    "    # Remove 'Missing' after explode just in case\n",
    "    tech_df = tech_df[tech_df[col] != 'Missing']\n",
    "\n",
    "    # Group and compute mean salary + count\n",
    "    salary_by_tech = (\n",
    "        tech_df.groupby(col)['ConvertedCompYearly']\n",
    "        .agg(mean_salary='mean', count='count')\n",
    "        .sort_values(by='count', ascending=False)\n",
    "        .head(10)\n",
    "        .sort_values(by='mean_salary', ascending=False)  # sort top 10 by salary\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    sns.barplot(data=salary_by_tech, x=col, y='mean_salary', palette='viridis')\n",
    "    plt.title(f\"Average Salary by {col.replace('HaveWorkedWith', '')} (Top 10 Most Used)\")\n",
    "    plt.ylabel(\"Mean Salary (USD)\")\n",
    "    plt.xlabel(col.replace('HaveWorkedWith', ''))\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIThreat to developer roles\n",
    "Although we did not use this in our prediction model, we found it useful to visualise how Artificial Intelligence (AI) affects the different developer roles in the market and if any of them felt particularly threatened by the rise and sophistication of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter clean data\n",
    "valid_ai_responses = [\"Yes\", \"No\", \"I'm not sure\"]\n",
    "df_ai = df[df['AIThreat'].isin(valid_ai_responses)].copy()\n",
    "\n",
    "# Prepare DevType column\n",
    "df_ai = df_ai.dropna(subset=['DevType'])\n",
    "df_ai['DevType'] = df_ai['DevType'].str.split(';')\n",
    "df_ai = df_ai.explode('DevType')\n",
    "df_ai['DevType'] = df_ai['DevType'].str.strip()\n",
    "\n",
    "# Normalize role threat responses\n",
    "# Total count of each role overall\n",
    "role_totals = df_ai['DevType'].value_counts()\n",
    "\n",
    "# Function to calculate top 10 roles by percentage for a given threat category\n",
    "def get_top_roles(threat_label, color):\n",
    "    threat_df = df_ai[df_ai['AIThreat'] == threat_label]\n",
    "    role_counts = threat_df['DevType'].value_counts()\n",
    "    role_percent = (role_counts / role_totals * 100).dropna()\n",
    "    top_roles = role_percent.nlargest(10)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(\n",
    "        x=top_roles.values,\n",
    "        y=top_roles.index,\n",
    "        palette=color\n",
    "    )\n",
    "    plt.title(f\"Top 10 Roles Most Likely to Respond '{threat_label}' (by %)\")\n",
    "    plt.xlabel(\"Percentage of Role Respondents\")\n",
    "    plt.ylabel(\"Developer Role\")\n",
    "    plt.xlim(0, 100)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return top_roles\n",
    "\n",
    "# Step 4: Run for each category\n",
    "top_yes = get_top_roles(\"Yes\", \"Reds\")\n",
    "top_no = get_top_roles(\"No\", \"Greens\")\n",
    "top_unsure = get_top_roles(\"I'm not sure\", \"YlOrBr\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results we can see a small number of professionals feel threatened by the rise of AI to their roles. In contrast we see a large number of developers confident that AI will not be a threat to their roles which is encouraging to note. We also had a couple of developers that were unsure of their fate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sub-Saharan Africa Salary Distribution Boxplot\n",
    "ssa_countries = [\n",
    "    'Angola', 'Benin', 'Botswana', 'Burkina Faso', 'Burundi', 'Cape Verde', 'Cameroon',\n",
    "    'Central African Republic', 'Chad', 'Comoros', 'Democratic Republic of the Congo', 'Congo', 'Congo, Republic of the...',\n",
    "    'C√¥te d‚ÄôIvoire', \"C√¥te d'Ivoire\", 'Djibouti', 'Equatorial Guinea', 'Eritrea', 'Eswatini', 'Ethiopia', \n",
    "    'Gabon', 'Gambia', 'Ghana', 'Guinea', 'Guinea-Bissau', 'Kenya', 'Lesotho', 'Liberia',\n",
    "    'Madagascar', 'Malawi', 'Mali', 'Mauritania', 'Mauritius', 'Mozambique', 'Namibia',\n",
    "    'Niger', 'Nigeria', 'Rwanda', 'S√£o Tom√© and Pr√≠ncipe', 'Senegal', 'Seychelles', \n",
    "    'Sierra Leone', 'Somalia', 'South Africa', 'South Sudan', 'Sudan', 'Swaziland', 'Tanzania', \n",
    "    'Togo', 'Uganda', 'United Republic of Tanzania', 'Zambia', 'Zimbabwe'\n",
    "]\n",
    "# ‚úÖ Filter SSA responses\n",
    "ssa_freq = df[df['Country'].isin(ssa_countries)]['Country'].value_counts()\n",
    "\n",
    "# ‚úÖ Display sorted frequencies\n",
    "print(\"üìä Response Counts by Sub-Saharan Country:\\n\")\n",
    "print(ssa_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2Ô∏è‚É£ Filter to SSA countries with valid salary data\n",
    "ssa_df = df[\n",
    "    (df['Country'].isin(ssa_countries)) &\n",
    "    (df['ConvertedCompYearly'].notna())\n",
    "].copy()\n",
    "\n",
    "# Optional: Remove outliers beyond the 95th percentile\n",
    "ssa_df = ssa_df[\n",
    "    ssa_df['ConvertedCompYearly'] < ssa_df['ConvertedCompYearly'].quantile(0.95)\n",
    "]\n",
    "\n",
    "# 3Ô∏è‚É£ Get top 10 SSA countries by response count\n",
    "top_10_ssa_countries = ssa_df['Country'].value_counts().nlargest(10).index\n",
    "\n",
    "# 4Ô∏è‚É£ Filter dataset to top 10 SSA countries\n",
    "top_10_ssa_df = ssa_df[ssa_df['Country'].isin(top_10_ssa_countries)]\n",
    "\n",
    "# 5Ô∏è‚É£ Compute average salary and sort\n",
    "mean_salaries = (\n",
    "    top_10_ssa_df.groupby('Country')['ConvertedCompYearly']\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# 6Ô∏è‚É£ Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x=mean_salaries.values,\n",
    "    y=mean_salaries.index,\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "# ‚úÖ Annotate values\n",
    "for i, value in enumerate(mean_salaries.values):\n",
    "    plt.text(value + 500, i, f\"${int(value):,}\", va='center')\n",
    "    \n",
    "plt.title('üåç Average Developer Salary by SSA Country (Top 10)', fontsize=14)\n",
    "plt.xlabel('Average Salary (USD)')\n",
    "plt.ylabel('Country')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per this analysis, South Africa, Zimbabwe and Kenya emerged as the countries in Kenya with the heighest average developer salary based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse previous top_10_ssa_df (already filtered)\n",
    "# Ensure outliers are removed for better visualization\n",
    "filtered_box_df = top_10_ssa_df[\n",
    "    top_10_ssa_df['ConvertedCompYearly'] < top_10_ssa_df['ConvertedCompYearly'].quantile(0.95)\n",
    "]\n",
    "\n",
    "# Sort countries by median salary\n",
    "ordered_countries = (\n",
    "    filtered_box_df.groupby('Country')['ConvertedCompYearly']\n",
    "    .median()\n",
    "    .sort_values(ascending=False)\n",
    "    .index\n",
    ")\n",
    "\n",
    "# Plot the boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    data=filtered_box_df,\n",
    "    x='Country',\n",
    "    y='ConvertedCompYearly',\n",
    "    order=ordered_countries,\n",
    "    palette='viridis',\n",
    "    showfliers=False  # Removes extreme outliers\n",
    ")\n",
    "\n",
    "plt.title(\"üíº Salary Distribution by Country (Top 10 SSA)\", fontsize=14)\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Annual Salary (USD)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "South Africa has the highest annual compensation levels, with the top end reaching around $80,000. The rest of the countries predominantly fall between the $10,000 - $30,000 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Salary Distribution by Developer Role (DevType)\n",
    "# Filter: keep rows with salary and DevType info\n",
    "role_df = df[\n",
    "    df['ConvertedCompYearly'].notna() &\n",
    "    df['DevType'].notna() &\n",
    "    (df['DevType'] != 'Missing')\n",
    "].copy()\n",
    "\n",
    "# Limit to top 10 most common roles to make the plot readable\n",
    "top_roles = role_df['DevType'].value_counts().nlargest(10).index\n",
    "role_df = role_df[role_df['DevType'].isin(top_roles)]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(\n",
    "    data=role_df,\n",
    "    x='DevType',\n",
    "    y='ConvertedCompYearly',\n",
    "    showfliers=False\n",
    ")\n",
    "\n",
    "plt.title('Salary Distribution by Developer Role')\n",
    "plt.xlabel('Developer Role')\n",
    "plt.ylabel('Annual Salary (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, role_df['ConvertedCompYearly'].quantile(0.95))  # Clip top 5% to reduce skew\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The roles with the highest salary ranges are Engineering Manager, Data scientist/ machine learning specialist and DevOps Specialist. These roles seem to have the potential for the highest annual compensation, reaching up to around $175,000 or more.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üõ†Ô∏è Feature Engineering**  \n",
    "In this section, we'll create new features, clean up the data further, and prepare it for our model. This ensures our model understands the data better and can predict salaries more accurately. We aim to:    \n",
    "\n",
    "        -   Makes the data easier for the model to understand  \n",
    "        -   Highlights important patterns (e.g., experience level or tech skills)  \n",
    "        -   Helps address issues like missing or skewed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### üõ†Ô∏è Feature Engineering: Creating New Features  \n",
    "We're creating new data points (features) to help our model predict salaries better. For example:,\n",
    "- Counting how many programming languages, tools and platforms a developer knows,\n",
    "- Grouping countries into regions (e.g., sub-saharan Africa, North America),\n",
    "- Simplifying education levels into categories like 'Bachelor‚Äôs' or 'No degree' \n",
    "\n",
    "This makes it easier for our model to find patterns, like whether knowing more programming languages leads to higher pay.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a Working Copy of the Dataset and Select Relevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.copy()\n",
    "\n",
    "# Drop rows with missing salary\n",
    "df = df[df['ConvertedCompYearly'].notna()]\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = [\n",
    "    'ConvertedCompYearly', 'Country', 'EdLevel', 'YearsCodePro',\n",
    "    'Employment', 'OrgSize', 'RemoteWork', 'LanguageHaveWorkedWith',\n",
    "    'DevType', 'SurveyYear', 'ICorPM', 'Industry', \n",
    "    'PurchaseInfluence', 'PlatformHaveWorkedWith', 'ToolsTechHaveWorkedWith'\n",
    "]\n",
    "df = df[selected_columns]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a New Feature: Number of Programming Languages, Platforms and Tools Known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NumLanguages'] = df['LanguageHaveWorkedWith'].apply(lambda x: len(str(x).split(';')) if pd.notna(x) else 0)\n",
    "df['NumPlatforms'] = df['PlatformHaveWorkedWith'].apply(lambda x: len(str(x).split(';')) if pd.notna(x) else 0)\n",
    "df['NumTools'] = df['ToolsTechHaveWorkedWith'].apply(lambda x: len(str(x).split(';')) if pd.notna(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Simplify Education Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EdLevel'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify education\n",
    "def simplify_edlevel(edlevel):\n",
    "    if pd.isna(edlevel):\n",
    "        return 'Unknown'\n",
    "    edlevel = str(edlevel)\n",
    "    if 'Bachelor' in edlevel:\n",
    "        return 'Bachelor‚Äôs'\n",
    "    elif 'Master' in edlevel:\n",
    "        return 'Master‚Äôs'\n",
    "    elif 'Professional' in edlevel or 'Ph.D' in edlevel or 'Ed.D' in edlevel or 'MD' in edlevel or 'JD' in edlevel:\n",
    "        return 'Doctorate/Professional'\n",
    "    elif 'Associate' in edlevel:\n",
    "        return 'Associate'\n",
    "    elif 'Secondary school' in edlevel or 'Some college' in edlevel:\n",
    "        return 'Some College/Secondary'\n",
    "    elif 'Primary' in edlevel:\n",
    "        return 'Primary'\n",
    "    elif 'Something else' in edlevel:\n",
    "        return 'Other'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "df['EdLevel_Simplified'] = df['EdLevel'].apply(simplify_edlevel) \n",
    "df['EdLevel'].apply(simplify_edlevel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Grouping countires into regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of respondents per country\n",
    "df['Country'].value_counts(ascending= False).reset_index().head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep your selected top 10 SSA countries separate\n",
    "african_countries = ssa_countries\n",
    "\n",
    "# Define a function to assign regions\n",
    "def get_region(country):\n",
    "    if pd.isna(country): \n",
    "        return 'Unknown'\n",
    "    elif country == 'Kenya': \n",
    "        return 'Kenya'\n",
    "    elif country in african_countries: \n",
    "        return 'Sub Saharan Africa'\n",
    "    elif country in ['United States of America', 'Canada']: \n",
    "        return 'North America'\n",
    "    elif country in ['United Kingdom of Great Britain and Northern Ireland', 'Germany', 'France', 'Italy', 'Netherlands', 'Sweden', 'Switzerland', 'Austria', 'Belgium', 'Portugal',\n",
    "                    'Spain', 'Greece', 'Norway', 'Iceland', 'Denmark', 'Finland', 'Ireland', 'Estonia', 'Lithuania', 'Latvia', 'Luxembourg', 'Monaco']: \n",
    "        return 'Western & Northern Europe'\n",
    "    elif country in ['Argentina', 'Bolivia', 'Brazil', 'Chile', 'Colombia', 'Costa Rica', 'Cuba', 'Dominican Republic', 'Ecuador', \n",
    "                    'El Salvador', 'Guatemala', 'Honduras', 'Mexico', 'Nicaragua', 'Panama', 'Paraguay', 'Peru', 'Uruguay', 'Venezuela', 'Venezuela, Bolivarian Republic of...']: \n",
    "        return 'Latin America'\n",
    "    elif country in ['India', 'Pakistan', 'Bangladesh', 'Sri Lanka', 'Indonesia', 'Nepal', 'Vietnam', 'Philippines', 'Malaysia', 'Thailand', \n",
    "                    'Bhutan', 'Maldives', 'Brunei', 'Cambodia', 'Laos', 'Myanmar','Viet Nam']:\n",
    "        return 'South & Southeast Asia'\n",
    "    elif country in ['China', 'Japan', 'South Korea', 'Taiwan', 'Hong Kong', 'Hong Kong (S.A.R.)', 'Singapore', 'Macau', 'Mongolia', 'North Korea', 'Republic of Korea', \"Democratic People's Republic of Korea\"]: \n",
    "        return 'East Asia'\n",
    "    elif country in ['Armenia', 'Albenia','Armenia', 'Montenegro', 'Poland', 'Ukraine', 'Romania', 'Russian Federation', 'Serbia', 'Czech Republic', 'Slovakia', 'Hungary', 'Moldova', 'Republic of Moldova', 'Belarus', 'Bulgaria', 'Kosovo', 'Slovenia',\n",
    "                    'Croatia', 'Bosnia and Herzegovina', 'Kazakhstan', 'Uzbekistan', 'Albania', 'Azerbaijan', 'Kyrgyzstan', 'Afghanistan']:\n",
    "        return 'Eastern Europe & Central Asia'\n",
    "    elif country in ['Australia', 'New Zealand', 'Fiji']: \n",
    "        return 'Oceania'\n",
    "    elif country in ['Antigua and Barbuda', 'Bahamas', 'Barbados', 'Cuba', 'Dominica',' Dominican Republic',' Grenada', 'Haiti', 'Jamaica', \n",
    "                    'Saint Kitts and Nevis', 'Saint Lucia',' Saint Vincent',' The Grenadines', 'Trinidad and Tobago', 'Anguilla', 'Aruba', \n",
    "                    'British Virgin Islands', 'Cayman Islands', 'Puerto Rico']:\n",
    "        return 'Caribbean'\n",
    "    elif country in ['Algeria', 'Egypt', 'Libya', 'Morocco', 'Tunisia', 'Mauritania',\n",
    "                 'Bahrain', 'Iran, Islamic Republic of...', 'Iraq', 'Israel', 'Jordan', 'Kuwait', 'Lebanon', 'Oman', 'Cyprus',\n",
    "                 'Turkey', 'Georgia', 'Malta', 'Palestine', 'Qatar', 'Saudi Arabia', 'Syria', 'Syrian Arab Republic', 'United Arab Emirates', 'Yemen']: \n",
    "         return 'MENA'\n",
    "    else: return 'Other'\n",
    "\n",
    "# Apply the region mapping\n",
    "df['Region'] = df['Country'].apply(get_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Region'] == 'Other']['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Grouping Employment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer Employment\n",
    "def simplify_employment(employment):\n",
    "    if pd.isna(employment) or 'I prefer not to say' in str(employment):\n",
    "        return 'Unknown'\n",
    "    employment_list = str(employment).split(';')\n",
    "    priority_order = [\n",
    "        'Employed, full-time',\n",
    "        'Independent contractor, freelancer, or self-employed',\n",
    "        'Employed, part-time',\n",
    "        'Student, full-time',\n",
    "        'Student, part-time',\n",
    "        'Not employed, but looking for work',\n",
    "        'Not employed, and not looking for work',\n",
    "        'Retired'\n",
    "    ]\n",
    "    for status in priority_order:\n",
    "        if status in employment_list:\n",
    "            if status == 'Employed, full-time': return 'Full-time'\n",
    "            elif status == 'Independent contractor, freelancer, or self-employed': return 'Freelance'\n",
    "            elif status == 'Employed, part-time': return 'Part-time'\n",
    "            elif status in ['Student, full-time', 'Student, part-time']: return 'Student'\n",
    "            elif status == 'Not employed, but looking for work': return 'Unemployed'\n",
    "            elif status == 'Not employed, and not looking for work': return 'Unemployed'\n",
    "            elif status == 'Retired': return 'Retired'\n",
    "    return 'Unknown'\n",
    "df['Employment_Simplified'] = df['Employment'].apply(simplify_employment)\n",
    "df['Employment'].apply(simplify_employment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Ensuring the `YearsCodePro` is numeric and sorting them into bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert and Clean YearsCodePro\n",
    "df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n",
    "df['YearsCodePro'].fillna(df['YearsCodePro'].median(), inplace=True)\n",
    "\n",
    "# Binned experience\n",
    "def bin_experience(years):\n",
    "    if pd.isna(years): return 'Unknown'\n",
    "    years = float(years)\n",
    "    if years <= 3: return 'Beginner'\n",
    "    elif years <= 7: return 'Intermediate'\n",
    "    elif years <= 12: return 'Advanced'\n",
    "    else: return 'Expert'\n",
    "\n",
    "df['ExperienceLevel'] = df['YearsCodePro'].apply(bin_experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Grouping `OrgSize` into `small`, `medium` and `large`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_orgsize(size):\n",
    "    if pd.isna(size): return 'Unknown'\n",
    "    if 'fewer than 10' in size or '10 to 19' in size: return 'Small'\n",
    "    elif '20 to 99' in size or '100 to 499' in size: return 'Medium'\n",
    "    elif '500 to 999' in size or '1,000 or more' in size: return 'Large'\n",
    "    else: return 'Unknown'\n",
    "df['OrgSize_Simplified'] = df['OrgSize'].apply(simplify_orgsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Binary classification for whether the role is managerial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ICorPM'] = df['ICorPM'].apply(lambda x: 1 if 'Manager' in str(x) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Grouping Industries into simplified buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer Industry\n",
    "def simplify_industry(industry):\n",
    "    if pd.isna(industry):\n",
    "        return 'Unknown'\n",
    "    industry = str(industry).replace('Other:', 'Other')  # Normalize \"Other:\"\n",
    "    if any(keyword in industry for keyword in ['Information Services', 'IT', 'Software Development', 'Computer Systems Design', 'Internet, Telecomm']):\n",
    "        return 'Tech'\n",
    "    elif any(keyword in industry for keyword in ['Financial', 'Banking', 'Fintech', 'Insurance']):\n",
    "        return 'Finance'\n",
    "    elif any(keyword in industry for keyword in ['Manufacturing', 'Transportation', 'Supply Chain', 'Wholesale', 'Oil', 'Energy']):\n",
    "        return 'Manufacturing/Supply Chain'\n",
    "    elif any(keyword in industry for keyword in ['Retail', 'Consumer Services', 'Higher Education', 'Legal', 'Advertising', 'Media']):\n",
    "        return 'Services'\n",
    "    elif 'Healthcare' in industry:\n",
    "        return 'Healthcare'\n",
    "    elif 'Government' in industry:\n",
    "        return 'Government'\n",
    "    elif 'Other' in industry:\n",
    "        return 'Other'\n",
    "    return 'Unknown'\n",
    "\n",
    "df['Industry_Simplified'] = df['Industry'].apply(simplify_industry)\n",
    "df['Industry'].apply(simplify_industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Simplifying the `RemoteWork` category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify Remote Work Categories\n",
    "def simplify_remote(remote):\n",
    "    if pd.isna(remote):\n",
    "        return 'Unknown'\n",
    "    elif 'Remote' in remote:\n",
    "        return 'Remote'\n",
    "    elif 'Hybrid' in remote:\n",
    "        return 'Hybrid'\n",
    "    else:\n",
    "        return 'In-person'\n",
    "\n",
    "df['RemoteWork_Simplified'] = df['RemoteWork'].apply(simplify_remote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Log transformation on target variable for easier modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transform Salary \n",
    "df['Log_ConvertedCompYearly'] = np.log1p(df['ConvertedCompYearly'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Creating a new column `RegionIncomeLevel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region-based income level (World Bank style)\n",
    "income_map = {\n",
    "    'Kenya': 'Lower-Middle',\n",
    "    'Sub Saharan Africa': 'Lower-Middle',\n",
    "    'North America': 'High',\n",
    "    'Western & Northern Europe': 'High',\n",
    "    'Eastern Europe': 'Upper-Middle',\n",
    "    'South & Southeast Asia': 'Lower-Middle',\n",
    "    'East Asia': 'High',\n",
    "    'MENA': 'Upper-Middle',\n",
    "    'Latin America': 'Upper-Middle',\n",
    "    'Other': 'Unknown'\n",
    "}\n",
    "df['RegionIncomeLevel'] = df['Region'].map(income_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. Creating `Region_Remote` column\n",
    "You are creating a new feature called Region_Remote, which combines the values of:\n",
    "\n",
    "Region (e.g., \"Kenya\", \"North America\")\n",
    "RemoteWork_Simplified (e.g., \"Remote\", \"Hybrid\", \"On-site\")\n",
    "üéØ Purpose:\n",
    "You're helping the model answer questions like:\n",
    "\n",
    "üí≠ \"Does being remote in North America affect salary differently than being remote in Kenya?\"\n",
    "Because:\n",
    "\n",
    "A remote developer in North America might earn more due to access to high-paying markets.\n",
    "A remote developer in Kenya might still earn less due to location-based bias or fewer global clients.\n",
    "By combining Region and RemoteWork_Simplified, the model can learn these nuanced patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Interaction term: Region √ó Remote Work\n",
    "df['Region_Remote'] = df['Region'] + '_' + df['RemoteWork_Simplified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape after feature engineering: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîç What We Did  \n",
    "\n",
    "- Number of Languages: Counted how many programming languages, tools and platforms each developer knows to capture skill diversity.\n",
    "- Simplified Education: Grouped education levels into broader categories (e.g., Bachelor‚Äôs, Master‚Äôs) to make them easier to analyze.\n",
    "- Region Grouping: Organized countries into regions, keeping Kenya separate for focus, while grouping others like North America or Europe.\n",
    "- Years of Experience: Converted years of professional coding experience to numbers and filled in missing values with the median.\n",
    "- Remote Work: Simplified remote work status into Remote, Hybrid, or In-person.\n",
    "- Salary Transformation: Applied a logarithmic transformation to salaries to reduce the impact of extreme values (e.g., very high salaries).\n",
    "\n",
    "NOTE: Use the number of tools, platforms, and languages (counts) as the primary approach for 'PlatformHaveWorkedWith', 'ToolsTechHaveWorkedWith', and 'LanguageHaveWorkedWith'. Reason being:\n",
    "\n",
    "- Efficiency with Sparse Data: Counts (NumPlatforms, NumTools, NumLanguages) reduce dimensionality, mitigating overfitting risks in SSA/Kenya.\n",
    "- General Salary Proxy: Skill diversity correlates with experience and marketability, a reliable predictor across regions, including SSA.\n",
    "- Flexibility: You can validate this with feature importance (e.g., SHAP) and switch to specific types if counts show low impact.  \n",
    "**Hybrid Option: If SHAP later indicates high importance for specific skills (e.g., \"Python\" or \"AWS\"), we can engineer top-N categories (e.g., top 5 languages) as additional binary features. Starting with counts to establish a baseline.**\n",
    "\n",
    "These steps help our model focus on the most important patterns in the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **‚öñÔ∏è Handling Imbalanced Data**\n",
    "\n",
    "Our dataset has a lot more responses from some countries (like the US) than others (like Kenya). This can throw off the final result.\n",
    "\n",
    "To fix this, we use techniques to balance the data, so our model pays attention to all regions, especially smaller ones like Kenya.\n",
    "\n",
    "Since we‚Äôre predicting a number (salary), we can‚Äôt directly use oversampling techniques like SMOTE.\n",
    "Instead, we ensure balance by using stratified sampling and possibly sample weighting during modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of regions\n",
    "print(\"Region distribution:\\n\", df['Region'].value_counts())\n",
    "\n",
    "# Limit dataset to the top 10 developer roles to reduce noise\n",
    "top_roles = df['DevType'].value_counts().nlargest(10).index\n",
    "df = df[df['DevType'].isin(top_roles)]\n",
    "\n",
    "print(f\"Shape after filtering: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ What We Did\n",
    "- Checked how many responses come from each region to understand imbalance.\n",
    "- Focused on the top 10 developer roles to keep the data manageable and relevant.\n",
    "This helps ensure our model isn‚Äôt overwhelmed by less relevant data and can focus on key patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Splitting the Data**\n",
    " \n",
    "To make sure our model can predict salaries accurately for new data, we split our dataset into two parts:  \n",
    "\n",
    "- Training set (80%): Used to teach the model how to predict salaries.  \n",
    "- Test set (20%): Used to check how well the model performs on new data. \n",
    "\n",
    "This is like practicing for a test with most of the material but saving a few questions to see how you do without help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target\n",
    "features = [ 'EdLevel_Simplified', 'Region', 'ExperienceLevel', 'RegionIncomeLevel', 'Region_Remote',\n",
    "    'RemoteWork_Simplified', 'DevType', 'NumLanguages',\n",
    "    'Employment_Simplified', 'OrgSize_Simplified', 'ICorPM', 'Industry_Simplified', 'SurveyYear', \n",
    "    'NumTools', 'NumPlatforms'\n",
    "]\n",
    "target = 'Log_ConvertedCompYearly'\n",
    "\n",
    "# Split into features and target\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Stratified split by region\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=X['Region']\n",
    ")\n",
    "\n",
    "# Oversample Kenya in the training set\n",
    "kenya_data = X_train[X_train['Region'] == 'Kenya']\n",
    "kenya_labels = y_train[kenya_data.index]\n",
    "\n",
    "X_train_bal = pd.concat([X_train, kenya_data]*3, ignore_index=True)\n",
    "y_train_bal = pd.concat([y_train, kenya_labels]*3, ignore_index=True)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úÖ What We Did\n",
    "- Selected key features like education, region, and number of languages to predict salaries.\n",
    "- Used the log-transformed salary as our target to handle skewed data.\n",
    "- Split the dataset into 80% training and 20% testing while preserving regional balance.  \n",
    "\n",
    "This prepares our data for modeling in a way that generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìä Visualizing Key Features\n",
    "\n",
    "To understand our new features, let‚Äôs visualize how log salary varies by region and number of programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot: Log Salary by Region\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x='Region', y='Log_ConvertedCompYearly', showfliers=False)\n",
    "plt.title('Log Salary Distribution by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Log Annual Salary (USD)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot: Log Salary vs Number of Programming Languages\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(data=df, x='NumLanguages', y='Log_ConvertedCompYearly', alpha=0.5)\n",
    "plt.title('Log Salary vs Number of Programming Languages')\n",
    "plt.xlabel('Number of Programming Languages')\n",
    "plt.ylabel('Log Annual Salary (USD)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Graph 1: Log Salary Distribution by Region\n",
    "\n",
    "This **boxplot** compares the **log-transformed annual salary** of developers across different **global regions**.\n",
    "\n",
    "#### üîç What It Shows:\n",
    "- Each box represents the salary distribution within a region.\n",
    "\n",
    "#### ‚úÖ Key Insights:\n",
    "- **North America** has one of the highest median salaries and a narrower range, suggesting consistently high developer pay.\n",
    "- **Sub-Saharan Africa** and **North Africa** show lower median salaries with a wider range, indicating more salary variability (Kenya included).\n",
    "- **Western Europe** and **Oceania** also rank high in terms of median salaries.\n",
    "- The log scale helps normalize extreme salary values to make patterns clearer across regions.\n",
    "\n",
    "#### üí° Takeaway:\n",
    "Where a developer lives can significantly influence their salary. Developers in North America and Western Europe tend to earn more than those in regions like Sub-Saharan Africa or South Asia.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Graph 2: Log Salary vs Number of Programming Languages\n",
    "\n",
    "This **scatterplot** shows the relationship between how many **programming languages** a developer knows and their **log-transformed salary**.\n",
    "\n",
    "#### üîç What It Shows:\n",
    "- Each point represents a developer.\n",
    "- The x-axis is the number of programming languages they reported working with.\n",
    "- The y-axis is their log-transformed annual salary.\n",
    "\n",
    "#### ‚úÖ Key Insights:\n",
    "- There's a **slight upward trend** ‚Äî developers who know more languages tend to earn more.\n",
    "- Most developers fall within the **1‚Äì10 language range**, with highly varied salaries.\n",
    "- Beyond ~10 languages, the correlation becomes weaker and more scattered.\n",
    "- Some high-salary outliers exist across all levels, indicating other factors (e.g., region, job title, experience) also matter.\n",
    "\n",
    "#### üí° Takeaway:\n",
    "Knowing more programming languages can **improve earning potential**, but it‚Äôs **not the only factor**. Salary outcomes also depend on experience, region, remote work status, job role, and other attributes.\n",
    "\n",
    "---\n",
    "\n",
    "üìù These two visualizations help us understand how **location** and **skill diversity** relate to developer salaries. They're key for guiding how we structure our prediction model and feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è **Modeling Pipeline**\n",
    "\n",
    "### What is a Modeling Pipeline?\n",
    "A modeling pipeline is like a recipe for making predictions. It combines steps to prepare the data (like turning categories into numbers) and then uses a math tool (model) to predict salaries. We‚Äôll try a few models to see which one works best:\n",
    "- **Baseline (Linear Regression)**: A simple model that assumes salary changes in a straight-line way with things like experience or education.\n",
    "- **Random Forest**: A smarter model that looks at patterns in the data by combining many small predictions.\n",
    "- **XGBoost**: A powerful model that‚Äôs great for complex data like ours.\n",
    "\n",
    "We‚Äôll use a pipeline to make sure our data is prepared the same way for each model, avoiding mistakes.\n",
    "\n",
    "### Why It Matters\n",
    "- Ensures our predictions are consistent and fair.\n",
    "- Lets us compare different models to find the best one.\n",
    "- Saves time by automating data preparation and modeling.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['NumLanguages', 'NumTools', 'NumPlatforms']\n",
    "categorical_features = ['EdLevel_Simplified', 'Region', 'RemoteWork_Simplified', 'DevType', 'SurveyYear', 'Employment_Simplified', 'OrgSize_Simplified', 'ICorPM', 'Industry_Simplified', \n",
    "    'ExperienceLevel', 'RegionIncomeLevel', 'Region_Remote']\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{model_name} RMSE: {rmse:.4f}\")\n",
    "    print(f\"{model_name} R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    kenya_test = X_test[X_test['Region'] == 'Kenya']\n",
    "    if not kenya_test.empty:\n",
    "        y_pred_kenya = model.predict(kenya_test)\n",
    "        rmse_kenya = np.sqrt(mean_squared_error(y_test[kenya_test.index], y_pred_kenya))\n",
    "        print(f\"{model_name} Kenya RMSE: {rmse_kenya:.4f}\")\n",
    "    else:\n",
    "        print(f\"{model_name} Kenya RMSE: No Kenya data in test set\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: Linear Regression\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "lr_model = evaluate_model(lr_pipeline, X_train, X_test, y_train, y_test, \"Linear Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "rf_model = evaluate_model(rf_pipeline, X_train, X_test, y_train, y_test, \"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', XGBRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "xgb_model = evaluate_model(xgb_pipeline, X_train, X_test, y_train, y_test, \"XGBoost\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What We Did in Modeling  \n",
    "- Set Up a Pipeline: Created a system to prepare data (scaling numbers, encoding categories) and run models consistently.\n",
    "- Tried Three Models:\n",
    "    - Linear Regression: A simple model to get a starting point.\n",
    "    - Random Forest: A model that combines many predictions for better accuracy.\n",
    "    - XGBoost: A powerful model for complex patterns.  \n",
    "- Evaluated for Kenya: Checked how well each model predicts salaries for Kenyan developers.\n",
    "\n",
    "What‚Äôs a Good Score?\n",
    "- We‚Äôre measuring error using RMSE (Root Mean Squared Error) on the adjusted (log) salaries.\n",
    "- A good RMSE is 0.35‚Äì0.45, meaning predictions are within ¬±40% of the actual salary.\n",
    "- We also check the R2 score, which shows how much variation our model explains (closer to 1.0 is better)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Evaluation\n",
    "\n",
    "How We Judge Success\n",
    "We use RMSE to see how close our predictions are to actual salaries. Since salaries vary widely, we use log salaries to keep errors consistent. We also look at Kenya RMSE to ensure smaller regions are represented fairly.\n",
    "\n",
    "What the Results Mean\n",
    "- RMSE of 0.35‚Äì0.45: Acceptably close predictions.\n",
    "- Kenya RMSE: Crucial for localized accuracy.\n",
    "- R2 Score: Shows how well the model explains salary variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot actual vs predicted salaries for the best model (XGBoost)\n",
    "y_pred_xgb = xgb_pipeline.predict(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred_xgb, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Log Salary')\n",
    "plt.ylabel('Predicted Log Salary')\n",
    "plt.title('Actual vs Predicted Log Salaries (XGBoost)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Kenya-specific plot\n",
    "kenya_test = X_test[X_test['Region'] == 'Kenya']\n",
    "if not kenya_test.empty:\n",
    "    y_pred_kenya = xgb_pipeline.predict(kenya_test)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test[kenya_test.index], y_pred_kenya, alpha=0.5, color='green')\n",
    "    plt.plot(\n",
    "        [y_test[kenya_test.index].min(), y_test[kenya_test.index].max()],\n",
    "        [y_test[kenya_test.index].min(), y_test[kenya_test.index].max()],\n",
    "        'r--', lw=2\n",
    "    )\n",
    "    plt.xlabel('Actual Log Salary (Kenya)')\n",
    "    plt.ylabel('Predicted Log Salary (Kenya)')\n",
    "    plt.title('Actual vs Predicted Log Salaries for Kenya (XGBoost)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Kenya data in test set for plotting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What These Plots Show\n",
    "- Actual vs Predicted: Points close to the red line are accurate predictions.\n",
    "- Kenya Plot: Shows model performance specifically for Kenyan developers.\n",
    "\n",
    "---\n",
    "## üîç Model Interpretation (Explainability)\n",
    "\n",
    "Why Explain the Model?\n",
    "Our prediction tool is a black box. We use SHAP to understand which features (like region or experience) most influence predictions. This makes results trustworthy for decision-making.\n",
    "\n",
    "What is SHAP?\n",
    "SHAP (SHapley Additive exPlanations) shows how much each feature contributes to a prediction. For example, does being in Kenya increase or decrease salary predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP to explain the XGBoost model\n",
    "X_test_transformed = xgb_pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "cat_features_encoded = xgb_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "feature_names = numerical_features + list(cat_features_encoded)\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(xgb_pipeline.named_steps['regressor'])\n",
    "shap_values = explainer.shap_values(X_test_transformed)\n",
    "\n",
    "# SHAP summary\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names, max_display=10)\n",
    "plt.title('Feature Importance for Salary Predictions (XGBoost)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP for Kenya\n",
    "kenya_test = X_test[X_test['Region'] == 'Kenya']\n",
    "if not kenya_test.empty:\n",
    "    kenya_test_transformed = xgb_pipeline.named_steps['preprocessor'].transform(kenya_test)\n",
    "    shap_values_kenya = explainer.shap_values(kenya_test_transformed)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values_kenya, kenya_test_transformed, feature_names=feature_names, max_display=10)\n",
    "    plt.title('Feature Importance for Salary Predictions in Kenya (XGBoost)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No Kenya data in test set for SHAP analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What SHAP Tells Us\n",
    "- Feature Importance: Red = higher values that increase salary; Blue = values that reduce it.\n",
    "- Kenya-Specific: Lets us see if the same patterns apply locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# LightGBM Pipeline\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m      4\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m'\u001b[39m, lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(\n\u001b[0;32m      5\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      6\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m      7\u001b[0m         num_leaves\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m,\n\u001b[0;32m      8\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      9\u001b[0m     ))\n\u001b[0;32m     10\u001b[0m ])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# LightGBM Pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate \n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"MAE:  {mae:,.2f}\")\n",
    "print(f\"RMSE: {rmse:,.2f}\")\n",
    "print(f\"R¬≤:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract trained model\n",
    "regressor = model.named_steps['regressor']\n",
    "feature_names = model.named_steps['preprocessor'].get_feature_names_out()\n",
    "importances = regressor.feature_importances_\n",
    "\n",
    "# Show top 15\n",
    "feat_imp = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feat_imp.sort_values(by='Importance', ascending=False).head(20).plot(\n",
    "    kind='barh', x='Feature', y='Importance', title=\"Top 20 Feature Importances\", figsize=(10, 6)\n",
    ")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Top 3 Features**\n",
    "\n",
    "- **`num__NumTools`**  \n",
    "  ‚Üí Number of tools a developer uses (e.g., Git, Docker, etc.)  \n",
    "   Strongest indicator of salary level ‚Äî possibly because using more tools reflects broader expertise and adaptability.\n",
    "\n",
    "- **`num__NumLanguages`**  \n",
    "  ‚Üí Number of programming languages known  \n",
    "   Higher number of languages is linked to higher pay.\n",
    "\n",
    "- **`num__NumPlatforms`**  \n",
    "  ‚Üí Number of platforms a developer works on (e.g., web, mobile, desktop)  \n",
    "   Versatility across platforms increases value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Education Features**\n",
    "\n",
    "- **`cat__EdLevel_Simplified_Master‚Äôs`**  \n",
    "  ‚Üí Developer has a Master's degree  \n",
    "   Having a Master's degree contributes positively to salary, reflecting higher qualifications.\n",
    "\n",
    "- **`cat__EdLevel_Simplified_Some College/Secondary`**  \n",
    "  ‚Üí Developer has some college or secondary education  \n",
    "   This level of education shows a moderate impact on salary, possibly indicating a starting point in the career.\n",
    "\n",
    "- **`cat__EdLevel_Simplified_Bachelor‚Äôs`**  \n",
    "  ‚Üí Developer has a Bachelor‚Äôs degree  \n",
    "   A Bachelor's degree seems to be a key milestone in determining salary, though not as impactful as advanced degrees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we ensemble so as to blend the strengths of the previous models to give us a more balanced and reliable prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "#  Drop rows where the target is missing\n",
    "df_model = df_all.dropna(subset=['ConvertedCompYearly'])\n",
    "\n",
    "#  Define features and target\n",
    "X = df_model.drop(columns=['ConvertedCompYearly'])\n",
    "y = df_model['ConvertedCompYearly']\n",
    "\n",
    "#  Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Define the preprocessor with sparse_output=True to avoid memory explosion\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Preprocessing for numerical columns\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical columns\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "# Combine both\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, numerical_cols),\n",
    "    ('cat', cat_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "\n",
    "#  Define base learners\n",
    "base_learners = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('xgb', XGBRegressor(n_estimators=100, random_state=42)),\n",
    "    ('lgbm', LGBMRegressor(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# Define the meta learner\n",
    "meta_learner = RidgeCV()\n",
    "\n",
    "# Step 7: Assemble the stacking model pipeline\n",
    "stacking_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', StackingRegressor(\n",
    "        estimators=base_learners,\n",
    "        final_estimator=meta_learner,\n",
    "        passthrough=True))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "#  Predict and evaluate\n",
    "y_pred_stack = stacking_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred_stack)\n",
    "rmse = mean_squared_error(y_test, y_pred_stack, squared=False)\n",
    "r2 = r2_score(y_test, y_pred_stack)\n",
    "\n",
    "# Step 10: Print evaluation metrics\n",
    "print(\"‚úÖ Ensemble Evaluation Metrics:\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R¬≤:   {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:salary-env]",
   "language": "python",
   "name": "conda-env-salary-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
